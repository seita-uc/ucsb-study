Speech perception,
● Two competing theories of perception discussed in textbook
– motor theory
– general auditory theory
● Motor theory
– speech is perceived by "retrieving" the articulatory gestures (e.g.,
tongue movements) involved
● General auditory theory
– speech is perceived by extracting patterns from the sound, as we
might for any other kind of sound;
Motor theory,

The motor theory of speech perception is the hypothesis that people perceive spoken words by identifying the vocal tract gestures with which they are pronounced rather than by identifying the sound patterns that speech generates.

● There is some neurophysiological evidence that motor
areas of the brain are active in speech perception
● fMRI is a method measuring blood flow to different
areas of the brain
● fMRI studies show activation in motor areas when
listening to sounds
– the activation area is specific to particular sound
● this suggests motor cortex may be involved in speech
perception

● transcranial magnetic stimulation (TMS)
– "zap" particular area of brain with magnetic field
● TMS on motor cortex can influence ability to
distinguish sounds
● can also boost nerve activity in tongue muscles
– if done while hearing speech sounds
– or watching videos of people talking

One problem with motor theory is that some studies have
also found boosted motor activity in the leg or other body
parts when hearing speech sounds
– this doesn't make sense if the activity is supposed to be related
to modeling speech articulation
● As we will see next week, there are reasons to think there
may be broader connections between sensorimotor
cognition and language
– but still it is a bit of a puzzler if seemingly "random" motor
programs are activated along with language
;
Mirror neurons,
Establishing the existence of motor neurons requires implanting
electrodes in the brain
– researchers had done this with monkeys
– not considered ethical to do it to humans. . .
● More recently there have been reports of mirror neurons in humans
– in patients who were already undergoing brain surgery for epilepsy
● It remains unclear whether mirror neurons are a "system" and how
closely they are linked to the motor system
– they may just be some neurons that fire in certain situations
● But pretty clear there is activity in motor areas of the brain when
observing actions by others;
McGurk effect,
McGurk effect shows that visual information
can confuse our speech perception
– this suggests that we are not solely relying on the
actual sound, but are somehow modeling what the
speaker is doing with their mouth;
General auditory theory,
● General auditory theory of speech perception
holds that speech is not that special
● We hear speech sounds like other sounds
● We learn to process them and extract info
relevant for decoding phonemes, etc.

● There is evidence that humans can have
categorical perception for non-speech sounds
– e.g., plucked vs. bowed violin string
● This provides support for the idea that speech
perception is not super special
– i.e., not a "module"

Although called general auditory theory, it
might more accurately be called general
cognitive theory
– because it assumes more broad involvement of
pattern-recognition and top-down processes in
speech perception
– not just sound, but also other relevant information
;
Top-down vs bottom-up,
"Bottom-up" theories of perception assume that processing
begins with smallest units and combines those into larger units
– you don't know the word until you've figured out the phonemes
● "Top-down" theories assume that knowledge of larger units
affects processing of smaller ones
– your hearing of the phoneme depends on what word you think you're
hearing
● Most theories involve a bit of both
– but general auditory theory is more likely to embrace top-down
influence on perception of individual speech sounds;
Phonemic restoration effect,
● When a sound is clipped out of audio, people can often "hear" it anyway
– we "fill it in"
● This is presumably because top-down information allows us to infer it
● This also makes language robust
– if we had to hear absolutely every sound, it would be difficult to communicate in
many contexts
– noisy environment
– yelling from a distance
– mouth full
– etc.

● Effect seems to be more robust in longer
words
– more context to fill in the gap
● Also seems to be easier to fill in sound when
replaced with "noise" rather than silence
– perhaps more akin to real-world interference
;
Exemplar theory,
● Another approach to speech perception
● Every time you hear a sound, you "store" it
● You note its similarity to other sounds you
have heard before
● Over time, clusters emerge
● You may also store other information from
the context where you heard it

● Some evidence for this comes from studies showing
unexpected effects on perception
● Hay et al (2006) had participants listen to words that are
minimal pairs for certain sounds
– air/ear, bear/beer, etc.
● These sounds are very similar in New Zealand English
and getting more similar
● When hearing each word, they saw a photo of a person
who supposedly was saying the word

● Features of the photo seemed to influence how the words were heard
– photos of younger people induced more errors
– possibly because younger people pronounce the vowels more similarly,
hence people are "expecting" very similar sounds
● Other studies indicate people are better at recognizing words they
heard before if they hear them again from the same speaker
● This suggests that people may track a good deal of information
about sounds that we might not have expected
– perhaps you note everything about everyone who ever says anything to you!;
Speech production,
● It's speech perception in reverse!
● "first guess" model for perception would be bottom-up
– hear sounds, figure out words
● "first guess" model for production would be top-down
– decide word, then say sounds. . .
● could it really be so simple?
– three guesses. . .

● We can distinguish between "sequential"
models and "connectionist" models
● "Sequential" model posits one-directional
process from "idea" to sounds
● "Connectionist" model posits web of influences
between levels
;
Speech errors,
Category constraint
– it is very rare for speech errors to result in swapping words from different "parts of
speech"
– "put the trash in the paper"
– "I talked about him with that problem"
– not "put the in trash the paper"
– not "I problem with him about that talked"
● this does suggest there is some sort of "frame" into which words are slotted
– not just words being activated on their own, willy-nilly
– but could also be explained by very strong connections between words of same
part of speech (and inhibitory connections with different parts of speech)

Lexical bias effect
– speech errors of individual sounds are more likely to
produce real words than we would expect
– and very unlikely to produce impossible words
● "slap in the face" "flap in the face" →
● "participate in interdisciplinary collaboration" →
"participlate in. . ."
● not "participatpl"

Mixed errors
– erroneous word has shared sound and related meaning to
intended word
– perhaps a Freudian slip?
– "two sticks of bread" (meant "butter")
– "Steven Pinker" "stinker" →
– more common than expected
– seem to show sound and meaning are "active" in word
selection and may influence errors

Connectionist/spreading-activation models are somewhat
better at explaining such patterns in speech errors
● An "incorrect" sound will activate words that contain it,
making them more likely to "slip in"
● Words will activate words with similar meaning,
allowing them to slip in
● Phrases may activate other phrases
– "the society. . . whose place they took PLACE

● Connectionist model also more naturally accounts
for "false starts"
– individual words or fragments may reach "critical" level
of activation and be spoken before overall plan is
"ready"
– in sequential model, more difficult to see how this could
happen if entire utterance is planned first
● False starts and similar phenomena are pervasive in
language, so worth thinking about;
Speech production,
● We don't have a fully worked-out model of
how people produce speech
● Connectionist approaches are promising
– but even simpler sequential models are not bad
● A lot of language that people actually produce
doesn't fit traditional definitions of "sentence"
– but models have historically been based on that...;
Priming,
● A pervasive psychological phenomenon
● Exposure to prime influences response to target
– often a form of facilitation
– prime makes target easier to process
– can also be different sorts of influence
– prime causes people to somehow change their response
to target
● We'll see many examples of this;
Sound symbolism,
● What are the causes of such sound-symbolic patterns?
● Some have hypothesized neural basis
– overlap between language areas and sensorimotor areas
– related to synaesthesia?
– (this is one reason the paper you'll read also investigates shape/taste mappings)
● Some hypothesize "iconic" basis
– back vowels often rounded round lips round shape → →
– low vowels "larger" because mouth more open
– stops "smaller" because short duration, sonorants "larger" because longer
duration
● There is no clear explanation right now;



